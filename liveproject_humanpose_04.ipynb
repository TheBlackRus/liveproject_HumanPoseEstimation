{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liveproject_humanpose_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1k8ni1b4SBY1vd-sFaGKC_IYS6IhfgVuj",
      "authorship_tag": "ABX9TyNBy9vEKtmR9PXyOlq5OTgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheBlackRus/liveproject_HumanPoseEstimation/blob/master/liveproject_humanpose_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYuShJPCJdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9ja1d5LAma",
        "colab_type": "text"
      },
      "source": [
        "Download the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rserV5MP9BFU",
        "colab_type": "code",
        "outputId": "dce0260f-1f7b-408c-a05f-5d98a8e37fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# This code downloads the coco dataset from Amazon S3 in parallel.\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "#files = [ 'train2017.zip']#,'val2017.zip', 'annotations_trainval2017.zip']\n",
        "files = [ 'val2017.zip', 'annotations_trainval2017.zip']\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "def download_and_unzip_from_s3(file_name, bucket_name='fast-ai-coco'):\n",
        "    print(\"Downloading\", file_name)\n",
        "    s3.download_file(bucket_name, file_name, file_name)\n",
        "    print(\"Finished downloading\", file_name, \". Starting to unzip.\")\n",
        "    subprocess.run([\"unzip\", file_name])\n",
        "    print(\"Finished unzipping\", file_name)\n",
        "\n",
        "# Download in parallel\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "with multiprocessing.Pool(num_cpus) as p:\n",
        "    p.map(download_and_unzip_from_s3, files)\n",
        "\n",
        "print(\"Done transferring all datasets\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading val2017.zip\n",
            "Downloading annotations_trainval2017.zip\n",
            "Finished downloading annotations_trainval2017.zip . Starting to unzip.\n",
            "Finished downloading val2017.zip . Starting to unzip.\n",
            "Finished unzipping annotations_trainval2017.zip\n",
            "Finished unzipping val2017.zip\n",
            "Done transferring all datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkU1BOr0m5Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC40iIoILFxt",
        "colab_type": "text"
      },
      "source": [
        "Generate the input images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR0OCQZmc6aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the meta data of the validation set\n",
        "import json\n",
        "file_name = \"annotations//person_keypoints_val2017.json\"\n",
        "with open(file_name, 'r') as json_raw:\n",
        "    meta = json.load(json_raw)\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from matplotlib.pyplot import imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCV75IYxKd03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "#from matplotlib.pyplot import imshow\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "class HumanPoseDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, meta_path,train= False, transform=None):\n",
        "        \n",
        "        self.meta = self.load_meta(meta_path)\n",
        "        self.train = train\n",
        "        \n",
        "        self.img_annotations = self.filter_annotations(self.meta[\"annotations\"])\n",
        "        #self.img_info = meta[\"images\"]\n",
        "        #self.img_catogory = meta[\"categories\"][0][\"keypoints\"]\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def load_meta(self,path):\n",
        "      \n",
        "      with open(path, 'r') as json_raw:\n",
        "          meta = json.load(json_raw)\n",
        "      return meta\n",
        "\n",
        "    def filter_annotations(self,meta):\n",
        "      min_width = 48 #192 # 12\n",
        "      min_height = 64 # 256 # 16\n",
        "      print(\"LEN before filter\",len(meta))\n",
        "      annot = filter(lambda x: x[\"iscrowd\"] == 0 and \n",
        "                    x['bbox'][2]>= min_width and x['bbox'][3]>=min_height and\n",
        "                    any(map(lambda y: y>0, x[\"keypoints\"][2:-1:3])) # visible              \n",
        "                    ,meta)\n",
        "      annot = list(annot)\n",
        "      \"\"\"\n",
        "      cleaned_annot = []\n",
        "      for sample in meta:\n",
        "        no_crowd = sample[\"iscrowd\"] == 0\n",
        "        start_x,start_y, w,h = sample['bbox']\n",
        "        not_too_small = w>= min_width and h>=min_height\n",
        "        inside_box = False\n",
        "        #print(sample[\"keypoints\"])\n",
        "        keypoints = sample[\"keypoints\"]\n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          kx,ky = keypoint\n",
        "          if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "            inside_box = True\n",
        "            break\n",
        "        if all([no_crowd, not_too_small, inside_box]):\n",
        "          cleaned_annot.append(sample)\n",
        "        \"\"\"\n",
        "      print(\"AFTER\", len(annot))\n",
        "      return annot #cleaned_annot\n",
        "\n",
        "    def __len__(self):\n",
        "        #print(len(self.img_annotations), \"anot\")\n",
        "        return len(self.img_annotations)\n",
        "\n",
        "    def get_image_name(self,annot):\n",
        "      \n",
        "      img_file = str(annot[\"image_id\"])\n",
        "      img_file = img_file.zfill(12) +\".jpg\" #img_info[\"file_name\"]\n",
        "      if self.train:\n",
        "        return \"train2017//\"+img_file\n",
        "      else:   \n",
        "        return \"val2017//\"+img_file\n",
        "\n",
        "    def resize_image(self,img, target_width=192, target_height=256):\n",
        "      img_resized = cv2.resize(img,(target_width,target_height))   \n",
        "      return img_resized\n",
        "\n",
        "    def crop_image(self,img, upper_left_corner, size):\n",
        "      #print(upper_left_corner,size, img.shape)\n",
        "      start_x, start_y = upper_left_corner\n",
        "      w,h = size\n",
        "      #print(\"CROP\",int(start_y),(int(start_y+h-1)),int(start_x),int(start_x+w-1), img.shape)\n",
        "      img_cropped = img[int(start_y):(int(start_y+h)),int(start_x):int(start_x+w),:]\n",
        "      #print(\"CROP\", img.shape)\n",
        "      return img_cropped\n",
        "\n",
        "    def adjust_keypoint(self,keypoint, start, scale):\n",
        "      keypoint_x, keypoint_y= keypoint\n",
        "      start_x,start_y = start\n",
        "      sx,sy = scale\n",
        "      x = (keypoint_x-start_x)*sx\n",
        "      y = (keypoint_y-start_y)*sy\n",
        "      return x,y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        annot = self.img_annotations[idx]\n",
        "        keypoints = annot[\"keypoints\"]\n",
        "\n",
        "        img_name = self.get_image_name(annot)\n",
        "        image = io.imread(img_name)\n",
        "        #\n",
        "        #print(\"IDX\",idx)\n",
        "        if len(image.shape) <3:\n",
        "          # thats a freakin gray image\n",
        "          image = np.dstack((image, image,image))\n",
        "        #plt.imshow(image)\n",
        "        target_width = 192\n",
        "        target_height = 256\n",
        "\n",
        "        #print(annot[\"bbox\"],\"box\")\n",
        "        start_x,start_y, w,h = annot[\"bbox\"]\n",
        "        #print(\"box\",annot[\"bbox\"])\n",
        "        #print(image.shape,\"ishape\")\n",
        "        img_crop = self.crop_image(image, (start_x,start_y),(w,h))\n",
        "        #print(\"crop\",img_crop.shape)\n",
        "        #plt.imshow(img_crop)\n",
        "        img_crop = self.resize_image(img_crop,target_width,target_height)\n",
        "        \n",
        "        img = img_crop / 255.0\n",
        "        img = (img-self.mean)/self.std\n",
        "        # imgshape (256, 192, 3)\n",
        "        #print(\"img\",img.shape)\n",
        "        img = np.transpose(img,[2,0,1]) # channel first\n",
        "\n",
        "        #img_batch = torch.Tensor([img, img])\n",
        "        #print(img_batch.shape)\n",
        "        #img_batch = img_batch.permute([0,3,2,1])\n",
        "\n",
        "        sx = target_width/w \n",
        "        sy = target_height/h\n",
        "        #print(\"sxsy\",sx,sy)\n",
        "\n",
        "        validity = annot[\"keypoints\"][2::3]\n",
        "\n",
        "        validity = np.array(validity)\n",
        "        validity = validity > 0\n",
        "        validity = validity.astype(np.int)\n",
        "        #print(validity.shape)\n",
        "\n",
        "        heatmaps = np.zeros((17,64,48))\n",
        "\n",
        "        keypoints_adjusted = []\n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          visible = keypoints[i*3+2]\n",
        "          #print(keypoint)\n",
        "          if visible:\n",
        "            x,y = self.adjust_keypoint(keypoint,(start_x,start_y),(sx,sy))\n",
        "            #print(\"XY\", x,y,int(y//4),int(x//4),sx,sy )\n",
        "            #not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h))\n",
        "            kx , ky = keypoint\n",
        "            #if kx< start_x or ky < start_y or kx> (start_x+w) or ky > (start_y +h):\n",
        "            if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "              validity[i] = 0\n",
        "              #print(\"FAIL\", start_x, kx, start_x+w, start_y,y, start_y+h)\n",
        "            else:\n",
        "              \n",
        "              #if (y//4 >= 64 or x//4 >=47):\n",
        "              #  print(\"fail\",idx,x,y)\n",
        "              heatmaps[i,int(y//4),int(x//4)] = 1\n",
        "              heatmaps[i,:,:] =  gaussian_filter(heatmaps[i,:,:], sigma=2)\n",
        "              #print(heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "              heatmaps[i,:,:] -= heatmaps[i,:,:].min() \n",
        "              heatmaps[i,:,:] /= heatmaps[i,:,:].max()\n",
        "            #print(\"N\",heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "            #plt.matshow(heatmaps[i,:,:])\n",
        "            #print(meta[\"categories\"][0][\"keypoints\"][i])\n",
        "          else:\n",
        "            x,y = (-1,-1)\n",
        "          keypoints_adjusted.append(x)\n",
        "          keypoints_adjusted.append(y)\n",
        "          \n",
        "        \"\"\"\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \"\"\"\n",
        "        return torch.Tensor(img), torch.Tensor(heatmaps), torch.Tensor(validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDRYnRjAmZKp",
        "colab_type": "code",
        "outputId": "d8570ade-6cc7-4b42-d581-372e40169443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data_val = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\")\n",
        "data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_train2017.json\", train=True)\n",
        "#data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\", train=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEN before filter 11004\n",
            "AFTER 4795\n",
            "LEN before filter 262465\n",
            "AFTER 112208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FAGMqogKgN0",
        "colab_type": "code",
        "outputId": "2e414a3b-b05c-4d12-a429-76e3b33d9f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(data_val)):\n",
        "  print(i)\n",
        "  _, labels, _ = data_val[i]\n",
        "  \n",
        "  # h in range(labels.shape[0]):\n",
        "  #  plt.matshow(labels[h,:,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "979 anot\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEMEXxTqxrOD",
        "colab_type": "code",
        "outputId": "430f577a-d27f-4b76-c07a-a4df4a9315c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_val[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2.1462,  2.1462,  2.1462,  ...,  1.9749,  1.9407,  1.9749],\n",
              "          [ 2.1119,  2.1119,  2.1290,  ...,  1.9749,  1.9064,  1.9749],\n",
              "          [ 2.1119,  2.1119,  2.1119,  ...,  2.0092,  1.9578,  2.0092],\n",
              "          ...,\n",
              "          [-1.0733, -0.8335, -0.8335,  ...,  2.1633,  2.1633,  2.1462],\n",
              "          [-0.7137, -0.9192, -1.2959,  ...,  2.1462,  2.1119,  2.1633],\n",
              "          [-1.6727, -1.4672, -1.3644,  ...,  2.0605,  2.0263,  2.1462]],\n",
              " \n",
              "         [[ 2.2710,  2.2710,  2.2710,  ...,  1.9909,  1.9734,  1.9734],\n",
              "          [ 2.2360,  2.2360,  2.2535,  ...,  1.9909,  1.9559,  1.9909],\n",
              "          [ 2.2360,  2.2360,  2.2360,  ...,  2.0259,  1.9909,  2.0084],\n",
              "          ...,\n",
              "          [-1.2129, -0.9678, -0.8452,  ...,  2.2185,  2.2185,  2.2185],\n",
              "          [-1.1429, -1.2129, -1.2654,  ...,  2.2535,  2.2360,  2.2360],\n",
              "          [-1.8081, -1.6506, -1.3179,  ...,  2.1660,  2.1660,  2.2010]],\n",
              " \n",
              "         [[ 2.3611,  2.3611,  2.3611,  ...,  2.0474,  2.0300,  2.0300],\n",
              "          [ 2.3263,  2.3263,  2.3437,  ...,  2.0474,  1.9951,  2.0300],\n",
              "          [ 2.3263,  2.3263,  2.3263,  ...,  2.0823,  2.0474,  2.0648],\n",
              "          ...,\n",
              "          [-1.1421, -0.8807, -0.7587,  ...,  2.2217,  2.2043,  2.2217],\n",
              "          [-1.0724, -1.0898, -1.1596,  ...,  2.2391,  2.1694,  2.2391],\n",
              "          [-1.6476, -1.4559, -1.1770,  ...,  2.1520,  2.1171,  2.2043]]]),\n",
              " tensor([[[1.0000, 0.7910, 0.4947,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.7910, 0.6257, 0.3913,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.4947, 0.3913, 0.2447,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[1.0000, 0.7910, 0.4947,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.7910, 0.6257, 0.3913,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.4947, 0.3913, 0.2447,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[1.0000, 0.7910, 0.4947,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.7910, 0.6257, 0.3913,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.4947, 0.3913, 0.2447,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
              " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvH0QY2pPI5C",
        "colab_type": "code",
        "outputId": "357ed0ea-e557-4b48-e7e4-ea2f458e6e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "meta.keys()\n",
        "print(meta[\"categories\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'supercategory': 'person', 'id': 1, 'name': 'person', 'keypoints': ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle'], 'skeleton': [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8], [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-DGF4yPKWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = 0\n",
        "img_info = meta[\"images\"][example]\n",
        "img_annotation = meta[\"annotations\"][example]\n",
        "img_catogory = meta[\"categories\"][0][\"keypoints\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z328qFkfbpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiP5YVWhd7Ag",
        "colab_type": "code",
        "outputId": "93eb9add-d3f5-44e6-a71c-5a36aff365a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #block\n",
        "        self.conv_1 = nn.Conv2d(in_channels = 3, out_channels=64,kernel_size=7,stride=2,padding=3)\n",
        "        #nn.init.normal_(self.conv_1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_1.bias,0)\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(64) # same as out_channels before layer \n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.pool_1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(in_channels = 64, out_channels=128,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_2.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_2.bias,0)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128) # same as out_channels before layer \n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.pool_2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv_3 = nn.Conv2d(in_channels= 128, out_channels=256,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_3.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_3.bias,0)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.pool_3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # lazy explanition in the workflow...\n",
        "        # right before\n",
        "        self.up1 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up1.weight, std=0.001)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up2.weight, std=0.001)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_channels= 256, out_channels=17,kernel_size=1,stride=1,padding=0) #nn.Linear(24 * 4 * 4, 10)# 24 chans x 32//(2*2*2)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "        #nn.init.normal_(self.fc1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.fc1.bias,0)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.pool_1(x)\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.pool_2(x)\n",
        "\n",
        "        x = self.conv_3(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.pool_3(x)\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        x = self.relu_4(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.batch_norm_5(x)\n",
        "        x = self.relu_5(x)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        x = self.fc1(x)#x.view(-1, 24 * 4 * 4)\n",
        "        x = self.sig(x)\n",
        "        \n",
        "        #x = F.relu(self.fc1(x))\n",
        "        #x = F.relu(self.fc2(x))\n",
        "        #x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv_1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "  (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_1): ReLU()\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (batch_norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_2): ReLU()\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (batch_norm_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_3): ReLU()\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (up1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (batch_norm_4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_4): ReLU()\n",
            "  (up2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (batch_norm_5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_5): ReLU()\n",
            "  (fc1): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_PsGUbpggVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torchsummary import summary\n",
        "#summary(net, (3, 256, 192))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm47vdrpn3dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img, heatmaps, validity = data_val[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrmuzFt9gsJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_loss(output, target,validity):\n",
        "    #print(\"out\",output.shape)\n",
        "    #print(\"tar\",target.shape)\n",
        "    #print(\"val\",validity.shape)\n",
        "    \"\"\"\n",
        "    for b in range(output.shape[0]):\n",
        "      for i in range(len(validity[b])):\n",
        "        torch.\n",
        "        output[b,i,:,:] = output[b,i,:,:] *validity[b,i]\n",
        "        target[b,i,:,:] = target[b,i,:,:] *validity[b,i]\n",
        "\n",
        "    loss = torch.mean((output - target)**2)\n",
        "    \"\"\"\n",
        "    #print(\"-\"*70)\n",
        "    #print((output-target).shape)\n",
        "    #print(\"x\"*50)\n",
        "    #print(validity.shape)\n",
        "     \n",
        "    loss = torch.mean(((output - target))**2,(2,3))\n",
        "    #print(\"los\",loss.shape)\n",
        "    #print(\"val\",(loss*validity).shape)\n",
        "    loss = torch.mean( torch.mul(  loss,validity))\n",
        "    \n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye8ysftErnV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_preds(batch_heatmaps):\n",
        "    '''\n",
        "    get predictions from score maps\n",
        "    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n",
        "    '''\n",
        "    #assert isinstance(batch_heatmaps, np.ndarray), \\\n",
        "    #    'batch_heatmaps should be numpy.ndarray'\n",
        "    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
        "\n",
        "    batch_size = batch_heatmaps.shape[0]\n",
        "    num_joints = batch_heatmaps.shape[1]\n",
        "    width = batch_heatmaps.shape[3]\n",
        "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n",
        "    idx = np.argmax(heatmaps_reshaped, 2)\n",
        "    maxvals = np.amax(heatmaps_reshaped, 2)\n",
        "\n",
        "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n",
        "    idx = idx.reshape((batch_size, num_joints, 1))\n",
        "\n",
        "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
        "\n",
        "    preds[:, :, 0] = (preds[:, :, 0]) % width\n",
        "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n",
        "\n",
        "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n",
        "    pred_mask = pred_mask.astype(np.float32)\n",
        "\n",
        "    preds *= pred_mask\n",
        "    return preds, maxvals\n",
        "\n",
        "\n",
        "def calc_dists(preds, target, normalize):\n",
        "    preds = preds.astype(np.float32)\n",
        "    target = target.astype(np.float32)\n",
        "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
        "    for n in range(preds.shape[0]):\n",
        "        for c in range(preds.shape[1]):\n",
        "            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n",
        "                normed_preds = preds[n, c, :] / normalize[n]\n",
        "                normed_targets = target[n, c, :] / normalize[n]\n",
        "                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n",
        "            else:\n",
        "                dists[c, n] = -1\n",
        "    return dists\n",
        "\n",
        "\n",
        "def dist_acc(dists, thr=0.5):\n",
        "    ''' Return percentage below threshold while ignoring values with a -1 '''\n",
        "    dist_cal = np.not_equal(dists, -1)\n",
        "    num_dist_cal = dist_cal.sum()\n",
        "    if num_dist_cal > 0:\n",
        "        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def calc_accuracy(output, target, hm_type='gaussian', thr=0.5):\n",
        "    '''\n",
        "    Calculate accuracy according to PCK,\n",
        "    but uses ground truth heatmap rather than x,y locations\n",
        "    First value to be returned is average accuracy across 'idxs',\n",
        "    followed by individual accuracies\n",
        "    '''\n",
        "    idx = list(range(output.shape[1]))\n",
        "    norm = 1.0\n",
        "    if hm_type == 'gaussian':\n",
        "        pred, _ = get_max_preds(output)\n",
        "        target, _ = get_max_preds(target)\n",
        "        h = output.shape[2]\n",
        "        w = output.shape[3]\n",
        "        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n",
        "    dists = calc_dists(pred, target, norm)\n",
        "\n",
        "    acc = np.zeros((len(idx) + 1))\n",
        "    avg_acc = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(len(idx)):\n",
        "        acc[i + 1] = dist_acc(dists[idx[i]])\n",
        "        if acc[i + 1] >= 0:\n",
        "            avg_acc = avg_acc + acc[i + 1]\n",
        "            cnt += 1\n",
        "\n",
        "    avg_acc = avg_acc / cnt if cnt != 0 else 0\n",
        "    if cnt != 0:\n",
        "        acc[0] = avg_acc\n",
        "    return avg_acc\n",
        "def get_accuracy(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    # TODO: calculate accuracy\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,validity = data\n",
        "      #labels = labels.squeeze()\n",
        "      inputs = inputs.cuda()\n",
        "     \n",
        "      \n",
        "      outputs = net(inputs)\n",
        "      \n",
        "      #print(predicted, labels)\n",
        "      o = outputs.cpu().detach().numpy()[:,:,:,:]\n",
        "      l = labels.detach().numpy()[:,:,:,:]\n",
        "      #plt.matshow(o[0,0,:,:])\n",
        "      #print(l.type)\n",
        "      a = calc_accuracy(o,l)\n",
        "      #print(\"a\",a)\n",
        "      acc +=a\n",
        "      count = count + 1\n",
        "    #print(\"AC\",acc,count)\n",
        "    #acc = acc.to(torch.float)\n",
        "    \n",
        "    if count >0:\n",
        "      accuracy = acc/count\n",
        "    else:\n",
        "      accuracy = -1\n",
        "    #outputs = net(inputs)\n",
        "    model.train()\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeV4wrmu76A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "criterion = my_loss\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.0001) # ,lr=0.001 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[10,100], gamma=0.1)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data_train, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(data_val, batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePTpjMDIUvuo",
        "colab_type": "code",
        "outputId": "a515eab2-4066-4b06-f313-aeb0ea7e529b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"device\",\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.cuda()\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "    print(\"EPOCH\",epoch)\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #print(\"-\"*60)\n",
        "        #print(\"datalen\",len(data))\n",
        "        inputs, labels, validity = data\n",
        "        #print(labels.shape)\n",
        "        #for b in range(labels.shape[0]):\n",
        "        #  for h in range(labels.shape[1]):\n",
        "        #    plt.matshow(labels[b,h,:,:])\n",
        "        #print(\"L\",labels.shape)\n",
        "        #labels = labels.squeeze()\n",
        "        #print(\"l\",labels.shape)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        validity = validity.cuda()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        outputs.cuda()\n",
        "        #print(labels,labels.shape)\n",
        "        loss = criterion(outputs, labels,validity)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        #print(\"#\"*60)\n",
        "        #if i % 100 == 10:    # print every 100 mini-batches\n",
        "    scheduler.step()\n",
        "    print('[%d, %5d] loss: %.7f' %(epoch + 1, i + 1, running_loss ))\n",
        "    acc = get_accuracy(net,testloader)\n",
        "    print(\"acc %.14f\" % acc)\n",
        "    running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device cuda\n",
            "EPOCH 0\n",
            "[1,  3507] loss: 20.2735727\n",
            "acc 0.32071281320975\n",
            "EPOCH 1\n",
            "[2,  3507] loss: 7.0666584\n",
            "acc 0.38308203489505\n",
            "EPOCH 2\n",
            "[3,  3507] loss: 6.6478848\n",
            "acc 0.41898041011279\n",
            "EPOCH 3\n",
            "[4,  3507] loss: 6.4030771\n",
            "acc 0.44165528246110\n",
            "EPOCH 4\n",
            "[5,  3507] loss: 6.2310237\n",
            "acc 0.45107220395456\n",
            "EPOCH 5\n",
            "[6,  3507] loss: 6.0975563\n",
            "acc 0.46454900812868\n",
            "EPOCH 6\n",
            "[7,  3507] loss: 5.9905730\n",
            "acc 0.47069109636303\n",
            "EPOCH 7\n",
            "[8,  3507] loss: 5.8975002\n",
            "acc 0.47390906441757\n",
            "EPOCH 8\n",
            "[9,  3507] loss: 5.8195777\n",
            "acc 0.48016981872986\n",
            "EPOCH 9\n",
            "[10,  3507] loss: 5.7479080\n",
            "acc 0.48335761313486\n",
            "EPOCH 10\n",
            "[11,  3507] loss: 5.6838441\n",
            "acc 0.48287028994149\n",
            "EPOCH 11\n",
            "[12,  3507] loss: 5.6268232\n",
            "acc 0.48592820942091\n",
            "EPOCH 12\n",
            "[13,  3507] loss: 5.5722530\n",
            "acc 0.48655302126171\n",
            "EPOCH 13\n",
            "[14,  3507] loss: 5.5237039\n",
            "acc 0.48837847373525\n",
            "EPOCH 14\n",
            "[15,  3507] loss: 5.4781577\n",
            "acc 0.48782826775548\n",
            "EPOCH 15\n",
            "[16,  3507] loss: 5.2411255\n",
            "acc 0.49711092004856\n",
            "EPOCH 16\n",
            "[17,  3507] loss: 5.1980484\n",
            "acc 0.49547665006097\n",
            "EPOCH 17\n",
            "[18,  3507] loss: 5.1793869\n",
            "acc 0.49445321718877\n",
            "EPOCH 18\n",
            "[19,  3507] loss: 5.1650713\n",
            "acc 0.49401088442319\n",
            "EPOCH 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QwignheXq0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}