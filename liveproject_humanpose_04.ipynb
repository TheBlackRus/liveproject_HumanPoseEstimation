{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liveproject_humanpose_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1k8ni1b4SBY1vd-sFaGKC_IYS6IhfgVuj",
      "authorship_tag": "ABX9TyOs04pGi+nOO/2tNA+EAmCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88663e9f4cfa41f29d4102784bf7d66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8500c8608ac948049dc20c61e4c354f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d081a22833a4a94a429b1d2b6d375c3",
              "IPY_MODEL_020f09342daa4210b31c17431ff8d2ac"
            ]
          }
        },
        "8500c8608ac948049dc20c61e4c354f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d081a22833a4a94a429b1d2b6d375c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14d6da42a7074ed7af2571206f5207f1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_908a02774a054177a39d6b22a7afa8ef"
          }
        },
        "020f09342daa4210b31c17431ff8d2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cda81a147b7545e3a3f38812c9039796",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:02&lt;00:00, 45.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ac8cee231c945058cf670acd0b93bbf"
          }
        },
        "14d6da42a7074ed7af2571206f5207f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "908a02774a054177a39d6b22a7afa8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cda81a147b7545e3a3f38812c9039796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ac8cee231c945058cf670acd0b93bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheBlackRus/liveproject_HumanPoseEstimation/blob/master/liveproject_humanpose_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYuShJPCJdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9ja1d5LAma",
        "colab_type": "text"
      },
      "source": [
        "Download the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rserV5MP9BFU",
        "colab_type": "code",
        "outputId": "21d6eb32-7408-44f5-dd21-9ba6f32b75c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# This code downloads the coco dataset from Amazon S3 in parallel.\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "#files = [ 'train2017.zip']#,'val2017.zip', 'annotations_trainval2017.zip']\n",
        "files = [ 'val2017.zip', 'annotations_trainval2017.zip']\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "def download_and_unzip_from_s3(file_name, bucket_name='fast-ai-coco'):\n",
        "    print(\"Downloading\", file_name)\n",
        "    s3.download_file(bucket_name, file_name, file_name)\n",
        "    print(\"Finished downloading\", file_name, \". Starting to unzip.\")\n",
        "    subprocess.run([\"unzip\", file_name])\n",
        "    print(\"Finished unzipping\", file_name)\n",
        "\n",
        "# Download in parallel\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "with multiprocessing.Pool(num_cpus) as p:\n",
        "    p.map(download_and_unzip_from_s3, files)\n",
        "\n",
        "print(\"Done transferring all datasets\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading val2017.zip\n",
            "Downloading annotations_trainval2017.zip\n",
            "Finished downloading annotations_trainval2017.zip . Starting to unzip.\n",
            "Finished downloading val2017.zip . Starting to unzip.\n",
            "Finished unzipping annotations_trainval2017.zip\n",
            "Finished unzipping val2017.zip\n",
            "Done transferring all datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkU1BOr0m5Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC40iIoILFxt",
        "colab_type": "text"
      },
      "source": [
        "Generate the input images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR0OCQZmc6aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the meta data of the validation set\n",
        "import json\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from matplotlib.pyplot import imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCV75IYxKd03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "#from matplotlib.pyplot import imshow\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "class HumanPoseDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, meta_path,train= False, transform=None):\n",
        "        \n",
        "        self.meta = self.load_meta(meta_path)\n",
        "        self.train = train\n",
        "        \n",
        "        self.img_annotations = self.filter_annotations(self.meta[\"annotations\"])\n",
        "        #self.img_info = meta[\"images\"]\n",
        "        #self.img_catogory = meta[\"categories\"][0][\"keypoints\"]\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def load_meta(self,path):\n",
        "      \n",
        "      with open(path, 'r') as json_raw:\n",
        "          meta = json.load(json_raw)\n",
        "      return meta\n",
        "\n",
        "    def filter_annotations(self,meta):\n",
        "      min_width = 192//4# 48 #192 # 12\n",
        "      min_height = 256//4#64 # 256 # 16\n",
        "      print(\"LEN before filter\",len(meta))\n",
        "      annot = filter(lambda x: x[\"iscrowd\"] == 0 and \n",
        "                    (x['bbox'][2]>= min_width and x['bbox'][3]>=min_height) and\n",
        "                    any(map(lambda y: y>0, x[\"keypoints\"][2:-1:3])) # visible              \n",
        "                    ,meta)\n",
        "      annot = list(annot)\n",
        "      \"\"\"\n",
        "      cleaned_annot = []\n",
        "      for sample in meta:\n",
        "        no_crowd = sample[\"iscrowd\"] == 0\n",
        "        start_x,start_y, w,h = sample['bbox']\n",
        "        not_too_small = w>= min_width and h>=min_height\n",
        "        inside_box = False\n",
        "        #print(sample[\"keypoints\"])\n",
        "        keypoints = sample[\"keypoints\"]\n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          kx,ky = keypoint\n",
        "          if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "            inside_box = True\n",
        "            break\n",
        "        if all([no_crowd, not_too_small, inside_box]):\n",
        "          cleaned_annot.append(sample)\n",
        "        \"\"\"\n",
        "      print(\"AFTER\", len(annot))\n",
        "      return annot #cleaned_annot\n",
        "\n",
        "    def __len__(self):\n",
        "        #print(len(self.img_annotations), \"anot\")\n",
        "        return len(self.img_annotations)\n",
        "\n",
        "    def get_image_name(self,annot):\n",
        "      \n",
        "      img_file = str(annot[\"image_id\"])\n",
        "      img_file = img_file.zfill(12) +\".jpg\" #img_info[\"file_name\"]\n",
        "      if self.train:\n",
        "        return \"train2017//\"+img_file\n",
        "      else:   \n",
        "        return \"val2017//\"+img_file\n",
        "\n",
        "    def resize_image(self,img, target_width=192, target_height=256):\n",
        "      img_resized = cv2.resize(img,(target_width,target_height))   \n",
        "      return img_resized\n",
        "\n",
        "    def crop_image(self,img, upper_left_corner, size):\n",
        "      #print(upper_left_corner,size, img.shape)\n",
        "      start_x, start_y = upper_left_corner\n",
        "      w,h = size\n",
        "      #print(\"CROP\",int(start_y),(int(start_y+h-1)),int(start_x),int(start_x+w-1), img.shape)\n",
        "      img_cropped = img[int(start_y):(int(start_y+h)),int(start_x):int(start_x+w),:]\n",
        "      #print(\"CROP\", img.shape)\n",
        "      return img_cropped\n",
        "\n",
        "    def adjust_keypoint(self,keypoint, start, scale):\n",
        "      keypoint_x, keypoint_y= keypoint\n",
        "      start_x,start_y = start\n",
        "      sx,sy = scale\n",
        "      x = (keypoint_x-start_x)*sx\n",
        "      y = (keypoint_y-start_y)*sy\n",
        "      return x,y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        annot = self.img_annotations[idx]\n",
        "        keypoints = annot[\"keypoints\"]\n",
        "\n",
        "        img_name = self.get_image_name(annot)\n",
        "        image = io.imread(img_name)\n",
        "        #\n",
        "        #print(\"IDX\",idx)\n",
        "        if len(image.shape) <3:\n",
        "          # thats a freakin gray image\n",
        "          image = np.dstack((image, image,image))\n",
        "        #plt.imshow(image)\n",
        "        target_width = 192\n",
        "        target_height = 256\n",
        "\n",
        "        #print(annot[\"bbox\"],\"box\")\n",
        "        start_x,start_y, w,h = annot[\"bbox\"]\n",
        "        #print(\"box\",annot[\"bbox\"])\n",
        "        #print(image.shape,\"ishape\")\n",
        "        img_crop = self.crop_image(image, (start_x,start_y),(w,h))\n",
        "        #print(\"crop\",img_crop.shape)\n",
        "        #plt.imshow(img_crop)\n",
        "        img_crop = self.resize_image(img_crop,target_width,target_height)\n",
        "        \n",
        "        img = img_crop / 255.0\n",
        "        img = (img-self.mean)/self.std\n",
        "        # imgshape (256, 192, 3)\n",
        "        #print(\"img\",img.shape)\n",
        "        img = np.transpose(img,[2,0,1]) # channel first\n",
        "\n",
        "        #img_batch = torch.Tensor([img, img])\n",
        "        #print(img_batch.shape)\n",
        "        #img_batch = img_batch.permute([0,3,2,1])\n",
        "\n",
        "        sx = target_width/w \n",
        "        sy = target_height/h\n",
        "        #print(\"sxsy\",sx,sy)\n",
        "\n",
        "        validity = annot[\"keypoints\"][2::3]\n",
        "\n",
        "        validity = np.array(validity)\n",
        "        validity = validity > 0\n",
        "        validity = validity.astype(np.int)\n",
        "        #print(validity.shape)\n",
        "\n",
        "        heatmaps = np.zeros((17,64,48))\n",
        "\n",
        "       \n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          visible = keypoints[i*3+2]\n",
        "          #print(keypoint)\n",
        "          \n",
        "          x,y = self.adjust_keypoint(keypoint,(start_x,start_y),(sx,sy))\n",
        "          #print(\"XY\", x,y,int(y//4),int(x//4),sx,sy )\n",
        "          #not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h))\n",
        "          kx , ky = keypoint\n",
        "          #if kx< start_x or ky < start_y or kx> (start_x+w) or ky > (start_y +h):\n",
        "          if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "            validity[i] = 0\n",
        "            #print(\"FAIL\", start_x, kx, start_x+w, start_y,y, start_y+h)\n",
        "          else:\n",
        "            \n",
        "            #if (y//4 >= 64 or x//4 >=47):\n",
        "            #  print(\"fail\",idx,x,y)\n",
        "            heatmaps[i,int(y//4),int(x//4)] = 1\n",
        "            heatmaps[i,:,:] =  gaussian_filter(heatmaps[i,:,:], sigma=2)\n",
        "            #print(heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "            heatmaps[i,:,:] -= heatmaps[i,:,:].min() \n",
        "            heatmaps[i,:,:] /= heatmaps[i,:,:].max()\n",
        "          #print(\"N\",heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "          #plt.matshow(heatmaps[i,:,:])\n",
        "          #print(meta[\"categories\"][0][\"keypoints\"][i])\n",
        "          \n",
        "          \n",
        "        \"\"\"\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \"\"\"\n",
        "        return torch.Tensor(img), torch.Tensor(heatmaps), torch.Tensor(validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzkwDttxFjR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDRYnRjAmZKp",
        "colab_type": "code",
        "outputId": "a7a0015b-4c7c-4012-a9a3-f2c81fbdff4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data_val = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\")\n",
        "data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_train2017.json\", train=True)\n",
        "#data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\", train=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEN before filter 11004\n",
            "AFTER 4795\n",
            "LEN before filter 262465\n",
            "AFTER 112208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FAGMqogKgN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(data_val)):\n",
        "  print(i)\n",
        "  _, labels, _ = data_val[i]\n",
        "  \n",
        "  # h in range(labels.shape[0]):\n",
        "  #  plt.matshow(labels[h,:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEMEXxTqxrOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_val[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z328qFkfbpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiP5YVWhd7Ag",
        "colab_type": "code",
        "outputId": "3fd3c43a-078c-4dd4-8ac2-b15e81dcb438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #block\n",
        "        self.conv_1 = nn.Conv2d(in_channels = 3, out_channels=64,kernel_size=7,stride=2,padding=3)\n",
        "        #nn.init.normal_(self.conv_1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_1.bias,0)\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(64) # same as out_channels before layer \n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.pool_1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(in_channels = 64, out_channels=128,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_2.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_2.bias,0)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128) # same as out_channels before layer \n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.pool_2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv_3 = nn.Conv2d(in_channels= 128, out_channels=256,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_3.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_3.bias,0)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.pool_3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # lazy explanition in the workflow...\n",
        "        # right before\n",
        "        self.up1 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up1.weight, std=0.001)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up2.weight, std=0.001)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_channels= 256, out_channels=17,kernel_size=1,stride=1,padding=0) #nn.Linear(24 * 4 * 4, 10)# 24 chans x 32//(2*2*2)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "        #nn.init.normal_(self.fc1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.fc1.bias,0)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.pool_1(x)\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.pool_2(x)\n",
        "\n",
        "        x = self.conv_3(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.pool_3(x)\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        x = self.relu_4(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.batch_norm_5(x)\n",
        "        x = self.relu_5(x)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        x = self.fc1(x)#x.view(-1, 24 * 4 * 4)\n",
        "        x = self.sig(x)\n",
        "        \n",
        "        #x = F.relu(self.fc1(x))\n",
        "        #x = F.relu(self.fc2(x))\n",
        "        #x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv_1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "  (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_1): ReLU()\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (batch_norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_2): ReLU()\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (batch_norm_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_3): ReLU()\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (up1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (batch_norm_4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_4): ReLU()\n",
            "  (up2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (batch_norm_5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_5): ReLU()\n",
            "  (fc1): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_PsGUbpggVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torchsummary import summary\n",
        "#summary(net, (3, 256, 192))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm47vdrpn3dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img, heatmaps, validity = data_val[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrmuzFt9gsJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_loss(output, target,validity):\n",
        "    #print(\"out\",output.shape)\n",
        "    #print(\"tar\",target.shape)\n",
        "    #print(\"val\",validity.shape)\n",
        "    \"\"\"\n",
        "    for b in range(output.shape[0]):\n",
        "      for i in range(len(validity[b])):\n",
        "        torch.\n",
        "        output[b,i,:,:] = output[b,i,:,:] *validity[b,i]\n",
        "        target[b,i,:,:] = target[b,i,:,:] *validity[b,i]\n",
        "\n",
        "    loss = torch.mean((output - target)**2)\n",
        "    \"\"\"\n",
        "    #print(\"-\"*70)\n",
        "    #print((output-target).shape)\n",
        "    #print(\"x\"*50)\n",
        "    #print(validity.shape)\n",
        "     \n",
        "    loss = torch.mean(((output - target))**2,(2,3))\n",
        "    #print(\"los\",loss.shape)\n",
        "    #print(\"val\",(loss*validity).shape)\n",
        "    loss = torch.mean( torch.mul(  loss,validity))\n",
        "    \n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye8ysftErnV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_preds(batch_heatmaps):\n",
        "    '''\n",
        "    get predictions from score maps\n",
        "    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n",
        "    '''\n",
        "    #assert isinstance(batch_heatmaps, np.ndarray), \\\n",
        "    #    'batch_heatmaps should be numpy.ndarray'\n",
        "    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
        "\n",
        "    batch_size = batch_heatmaps.shape[0]\n",
        "    num_joints = batch_heatmaps.shape[1]\n",
        "    width = batch_heatmaps.shape[3]\n",
        "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n",
        "    idx = np.argmax(heatmaps_reshaped, 2)\n",
        "    maxvals = np.amax(heatmaps_reshaped, 2)\n",
        "\n",
        "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n",
        "    idx = idx.reshape((batch_size, num_joints, 1))\n",
        "\n",
        "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
        "\n",
        "    preds[:, :, 0] = (preds[:, :, 0]) % width\n",
        "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n",
        "\n",
        "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n",
        "    pred_mask = pred_mask.astype(np.float32)\n",
        "\n",
        "    preds *= pred_mask\n",
        "    return preds, maxvals\n",
        "\n",
        "\n",
        "def calc_dists(preds, target, normalize):\n",
        "    preds = preds.astype(np.float32)\n",
        "    target = target.astype(np.float32)\n",
        "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
        "    for n in range(preds.shape[0]):\n",
        "        for c in range(preds.shape[1]):\n",
        "            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n",
        "                normed_preds = preds[n, c, :] / normalize[n]\n",
        "                normed_targets = target[n, c, :] / normalize[n]\n",
        "                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n",
        "            else:\n",
        "                dists[c, n] = -1\n",
        "    return dists\n",
        "\n",
        "\n",
        "def dist_acc(dists, thr=0.5):\n",
        "    ''' Return percentage below threshold while ignoring values with a -1 '''\n",
        "    dist_cal = np.not_equal(dists, -1)\n",
        "    num_dist_cal = dist_cal.sum()\n",
        "    if num_dist_cal > 0:\n",
        "        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def calc_accuracy(output, target, hm_type='gaussian', thr=0.5):\n",
        "    '''\n",
        "    Calculate accuracy according to PCK,\n",
        "    but uses ground truth heatmap rather than x,y locations\n",
        "    First value to be returned is average accuracy across 'idxs',\n",
        "    followed by individual accuracies\n",
        "    '''\n",
        "    idx = list(range(output.shape[1]))\n",
        "    norm = 1.0\n",
        "    if hm_type == 'gaussian':\n",
        "        pred, _ = get_max_preds(output)\n",
        "        target, _ = get_max_preds(target)\n",
        "        h = output.shape[2]\n",
        "        w = output.shape[3]\n",
        "        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n",
        "    dists = calc_dists(pred, target, norm)\n",
        "\n",
        "    acc = np.zeros((len(idx) + 1))\n",
        "    avg_acc = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(len(idx)):\n",
        "        acc[i + 1] = dist_acc(dists[idx[i]])\n",
        "        if acc[i + 1] >= 0:\n",
        "            avg_acc = avg_acc + acc[i + 1]\n",
        "            cnt += 1\n",
        "\n",
        "    avg_acc = avg_acc / cnt if cnt != 0 else 0\n",
        "    if cnt != 0:\n",
        "        acc[0] = avg_acc\n",
        "    return avg_acc\n",
        "def get_accuracy(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    # TODO: calculate accuracy\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    #for i, data in enumerate(dataloader, 0):\n",
        "    i =0\n",
        "    data = next(dataloader)\n",
        "    inputs, labels,validity = data\n",
        "    #labels = labels.squeeze()\n",
        "    inputs = inputs.cuda()\n",
        "    \n",
        "    \n",
        "    outputs = net(inputs)\n",
        "    \n",
        "    #print(predicted, labels)\n",
        "    o = outputs.cpu().detach().numpy()[:,:,:,:]\n",
        "    l = labels.detach().numpy()[:,:,:,:]\n",
        "    #plt.matshow(o[0,0,:,:])\n",
        "    #print(l.type)\n",
        "    a = calc_accuracy(o,l)\n",
        "    #print(\"a\",a)\n",
        "    acc +=a\n",
        "    count = count + 1\n",
        "    #print(\"AC\",acc,count)\n",
        "    #acc = acc.to(torch.float)\n",
        "    \n",
        "    if count >0:\n",
        "      accuracy = acc/count\n",
        "    else:\n",
        "      accuracy = -1\n",
        "    #outputs = net(inputs)\n",
        "    model.train()\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeV4wrmu76A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "criterion = my_loss\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.0001) # ,lr=0.001 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[5,100], gamma=0.1)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data_train, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(data_val, batch_size=32,\n",
        "                                          shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePTpjMDIUvuo",
        "colab_type": "code",
        "outputId": "62c09c68-1fe3-4857-fcf1-ad4e8a1a7193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "print(\"device\",\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "running_loss = 0.0\n",
        "eval_iter = iter(testloader)\n",
        "net.cuda()\n",
        "print(\"start training\")\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    \n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #print(\"-\"*60)\n",
        "        #print(\"datalen\",len(data))\n",
        "        inputs, labels, validity = data\n",
        "        #print(labels.shape)\n",
        "        #for b in range(labels.shape[0]):\n",
        "        #  for h in range(labels.shape[1]):\n",
        "        #    plt.matshow(labels[b,h,:,:])\n",
        "        #print(\"L\",labels.shape)\n",
        "        #labels = labels.squeeze()\n",
        "        #print(\"l\",labels.shape)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        validity = validity.cuda()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        outputs.cuda()\n",
        "        #print(labels,labels.shape)\n",
        "        loss = criterion(outputs, labels,validity)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        #print(\"#\"*60)\n",
        "        #if i % 100 == 10:    # print every 100 mini-batches\n",
        "    scheduler.step()\n",
        "    #print(\"EPOCH\",epoch)\n",
        "    acc = get_accuracy(net,eval_iter)\n",
        "    print('EPOCH [%3d, %5d] | loss: %.7f | acc %.14f' %(epoch, i + 1, running_loss, acc ))\n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device cuda\n",
            "start training\n",
            "EPOCH [  0,  3507] | loss: 19.2072622 | acc 0.30195623140897\n",
            "EPOCH [  1,  3507] | loss: 7.0061589 | acc 0.39658186200602\n",
            "EPOCH [  2,  3507] | loss: 6.6062977 | acc 0.37255693824579\n",
            "EPOCH [  3,  3507] | loss: 6.3698768 | acc 0.46632402356615\n",
            "EPOCH [  4,  3507] | loss: 6.2014352 | acc 0.43743019499742\n",
            "EPOCH [  5,  3507] | loss: 6.0711500 | acc 0.49007533220336\n",
            "EPOCH [  6,  3507] | loss: 5.9642977 | acc 0.45647043195201\n",
            "EPOCH [  7,  3507] | loss: 5.8757120 | acc 0.47581655772272\n",
            "EPOCH [  8,  3507] | loss: 5.7967115 | acc 0.46660789904511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6456ab357b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(\"-\"*60)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QwignheXq0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPhO_5fy72-",
        "colab_type": "text"
      },
      "source": [
        "4.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOe99gv8y91y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from torch.utils import load_state_dict_from_url\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        \n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        \n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(2048, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up1.weight, std=0.001)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up2.weight, std=0.001)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up3.weight, std=0.001)\n",
        "        self.batch_norm_6 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_6 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_channels= 256, out_channels=17,kernel_size=1,stride=1,padding=0) #nn.Linear(24 * 4 * 4, 10)# 24 chans x 32//(2*2*2)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.up1(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        x = self.relu_4(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.batch_norm_5(x)\n",
        "        x = self.relu_5(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = self.batch_norm_6(x)\n",
        "        x = self.relu_6(x)\n",
        "        \n",
        "        x = self.fc1(x)#x.view(-1, 24 * 4 * 4)\n",
        "        x = self.sig(x)\n",
        "        \n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT72-6HS0Mgr",
        "colab_type": "code",
        "outputId": "2cc9be27-9e66-4e40-9e23-6597ddd86044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "88663e9f4cfa41f29d4102784bf7d66d",
            "8500c8608ac948049dc20c61e4c354f6",
            "6d081a22833a4a94a429b1d2b6d375c3",
            "020f09342daa4210b31c17431ff8d2ac",
            "14d6da42a7074ed7af2571206f5207f1",
            "908a02774a054177a39d6b22a7afa8ef",
            "cda81a147b7545e3a3f38812c9039796",
            "1ac8cee231c945058cf670acd0b93bbf"
          ]
        }
      },
      "source": [
        "from torch.utils.model_zoo import load_url\n",
        "model = resnet50()\n",
        "state_dict = load_url(model_urls['resnet50'],progress=True)\n",
        "\n",
        "model.load_state_dict(state_dict,strict=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88663e9f4cfa41f29d4102784bf7d66d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['up1.weight', 'up1.bias', 'batch_norm_4.weight', 'batch_norm_4.bias', 'batch_norm_4.running_mean', 'batch_norm_4.running_var', 'up2.weight', 'up2.bias', 'batch_norm_5.weight', 'batch_norm_5.bias', 'batch_norm_5.running_mean', 'batch_norm_5.running_var', 'up3.weight', 'up3.bias', 'batch_norm_6.weight', 'batch_norm_6.bias', 'batch_norm_6.running_mean', 'batch_norm_6.running_var', 'fc1.weight', 'fc1.bias'], unexpected_keys=['fc.weight', 'fc.bias'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNnmuft4aNN",
        "colab_type": "text"
      },
      "source": [
        "Freeze layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_V6Whh3ybp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layer1.requires_grad_ = False\n",
        "model.layer2.requires_grad_ = False\n",
        "model.layer3.requires_grad_ = False\n",
        "model.layer4.requires_grad_ = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2ruYVor4RtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzjn9WV045Lo",
        "colab_type": "code",
        "outputId": "fcd851bf-85f2-4aaf-9882-e4a7394342f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "net.cuda()\n",
        "summary(net, (3, 256, 192))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 64, 128, 96]           9,408\n",
            "       BatchNorm2d-2          [-1, 64, 128, 96]             128\n",
            "              ReLU-3          [-1, 64, 128, 96]               0\n",
            "         MaxPool2d-4           [-1, 64, 64, 48]               0\n",
            "            Conv2d-5           [-1, 64, 64, 48]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 64, 48]             128\n",
            "              ReLU-7           [-1, 64, 64, 48]               0\n",
            "            Conv2d-8           [-1, 64, 64, 48]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 64, 48]             128\n",
            "             ReLU-10           [-1, 64, 64, 48]               0\n",
            "           Conv2d-11          [-1, 256, 64, 48]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 64, 48]             512\n",
            "           Conv2d-13          [-1, 256, 64, 48]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 64, 48]             512\n",
            "             ReLU-15          [-1, 256, 64, 48]               0\n",
            "       Bottleneck-16          [-1, 256, 64, 48]               0\n",
            "           Conv2d-17           [-1, 64, 64, 48]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 64, 48]             128\n",
            "             ReLU-19           [-1, 64, 64, 48]               0\n",
            "           Conv2d-20           [-1, 64, 64, 48]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 64, 48]             128\n",
            "             ReLU-22           [-1, 64, 64, 48]               0\n",
            "           Conv2d-23          [-1, 256, 64, 48]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 64, 48]             512\n",
            "             ReLU-25          [-1, 256, 64, 48]               0\n",
            "       Bottleneck-26          [-1, 256, 64, 48]               0\n",
            "           Conv2d-27           [-1, 64, 64, 48]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 64, 48]             128\n",
            "             ReLU-29           [-1, 64, 64, 48]               0\n",
            "           Conv2d-30           [-1, 64, 64, 48]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 64, 48]             128\n",
            "             ReLU-32           [-1, 64, 64, 48]               0\n",
            "           Conv2d-33          [-1, 256, 64, 48]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 64, 48]             512\n",
            "             ReLU-35          [-1, 256, 64, 48]               0\n",
            "       Bottleneck-36          [-1, 256, 64, 48]               0\n",
            "           Conv2d-37          [-1, 128, 64, 48]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 64, 48]             256\n",
            "             ReLU-39          [-1, 128, 64, 48]               0\n",
            "           Conv2d-40          [-1, 128, 32, 24]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 32, 24]             256\n",
            "             ReLU-42          [-1, 128, 32, 24]               0\n",
            "           Conv2d-43          [-1, 512, 32, 24]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 32, 24]           1,024\n",
            "           Conv2d-45          [-1, 512, 32, 24]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 32, 24]           1,024\n",
            "             ReLU-47          [-1, 512, 32, 24]               0\n",
            "       Bottleneck-48          [-1, 512, 32, 24]               0\n",
            "           Conv2d-49          [-1, 128, 32, 24]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 32, 24]             256\n",
            "             ReLU-51          [-1, 128, 32, 24]               0\n",
            "           Conv2d-52          [-1, 128, 32, 24]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 32, 24]             256\n",
            "             ReLU-54          [-1, 128, 32, 24]               0\n",
            "           Conv2d-55          [-1, 512, 32, 24]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 32, 24]           1,024\n",
            "             ReLU-57          [-1, 512, 32, 24]               0\n",
            "       Bottleneck-58          [-1, 512, 32, 24]               0\n",
            "           Conv2d-59          [-1, 128, 32, 24]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 32, 24]             256\n",
            "             ReLU-61          [-1, 128, 32, 24]               0\n",
            "           Conv2d-62          [-1, 128, 32, 24]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 32, 24]             256\n",
            "             ReLU-64          [-1, 128, 32, 24]               0\n",
            "           Conv2d-65          [-1, 512, 32, 24]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 32, 24]           1,024\n",
            "             ReLU-67          [-1, 512, 32, 24]               0\n",
            "       Bottleneck-68          [-1, 512, 32, 24]               0\n",
            "           Conv2d-69          [-1, 128, 32, 24]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 32, 24]             256\n",
            "             ReLU-71          [-1, 128, 32, 24]               0\n",
            "           Conv2d-72          [-1, 128, 32, 24]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 32, 24]             256\n",
            "             ReLU-74          [-1, 128, 32, 24]               0\n",
            "           Conv2d-75          [-1, 512, 32, 24]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 32, 24]           1,024\n",
            "             ReLU-77          [-1, 512, 32, 24]               0\n",
            "       Bottleneck-78          [-1, 512, 32, 24]               0\n",
            "           Conv2d-79          [-1, 256, 32, 24]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 32, 24]             512\n",
            "             ReLU-81          [-1, 256, 32, 24]               0\n",
            "           Conv2d-82          [-1, 256, 16, 12]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 16, 12]             512\n",
            "             ReLU-84          [-1, 256, 16, 12]               0\n",
            "           Conv2d-85         [-1, 1024, 16, 12]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 16, 12]           2,048\n",
            "           Conv2d-87         [-1, 1024, 16, 12]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 16, 12]           2,048\n",
            "             ReLU-89         [-1, 1024, 16, 12]               0\n",
            "       Bottleneck-90         [-1, 1024, 16, 12]               0\n",
            "           Conv2d-91          [-1, 256, 16, 12]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 16, 12]             512\n",
            "             ReLU-93          [-1, 256, 16, 12]               0\n",
            "           Conv2d-94          [-1, 256, 16, 12]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 16, 12]             512\n",
            "             ReLU-96          [-1, 256, 16, 12]               0\n",
            "           Conv2d-97         [-1, 1024, 16, 12]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 16, 12]           2,048\n",
            "             ReLU-99         [-1, 1024, 16, 12]               0\n",
            "      Bottleneck-100         [-1, 1024, 16, 12]               0\n",
            "          Conv2d-101          [-1, 256, 16, 12]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 16, 12]             512\n",
            "            ReLU-103          [-1, 256, 16, 12]               0\n",
            "          Conv2d-104          [-1, 256, 16, 12]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 16, 12]             512\n",
            "            ReLU-106          [-1, 256, 16, 12]               0\n",
            "          Conv2d-107         [-1, 1024, 16, 12]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 16, 12]           2,048\n",
            "            ReLU-109         [-1, 1024, 16, 12]               0\n",
            "      Bottleneck-110         [-1, 1024, 16, 12]               0\n",
            "          Conv2d-111          [-1, 256, 16, 12]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 16, 12]             512\n",
            "            ReLU-113          [-1, 256, 16, 12]               0\n",
            "          Conv2d-114          [-1, 256, 16, 12]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 16, 12]             512\n",
            "            ReLU-116          [-1, 256, 16, 12]               0\n",
            "          Conv2d-117         [-1, 1024, 16, 12]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 16, 12]           2,048\n",
            "            ReLU-119         [-1, 1024, 16, 12]               0\n",
            "      Bottleneck-120         [-1, 1024, 16, 12]               0\n",
            "          Conv2d-121          [-1, 256, 16, 12]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 16, 12]             512\n",
            "            ReLU-123          [-1, 256, 16, 12]               0\n",
            "          Conv2d-124          [-1, 256, 16, 12]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 16, 12]             512\n",
            "            ReLU-126          [-1, 256, 16, 12]               0\n",
            "          Conv2d-127         [-1, 1024, 16, 12]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 16, 12]           2,048\n",
            "            ReLU-129         [-1, 1024, 16, 12]               0\n",
            "      Bottleneck-130         [-1, 1024, 16, 12]               0\n",
            "          Conv2d-131          [-1, 256, 16, 12]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 16, 12]             512\n",
            "            ReLU-133          [-1, 256, 16, 12]               0\n",
            "          Conv2d-134          [-1, 256, 16, 12]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 16, 12]             512\n",
            "            ReLU-136          [-1, 256, 16, 12]               0\n",
            "          Conv2d-137         [-1, 1024, 16, 12]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 16, 12]           2,048\n",
            "            ReLU-139         [-1, 1024, 16, 12]               0\n",
            "      Bottleneck-140         [-1, 1024, 16, 12]               0\n",
            "          Conv2d-141          [-1, 512, 16, 12]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 16, 12]           1,024\n",
            "            ReLU-143          [-1, 512, 16, 12]               0\n",
            "          Conv2d-144            [-1, 512, 8, 6]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 8, 6]           1,024\n",
            "            ReLU-146            [-1, 512, 8, 6]               0\n",
            "          Conv2d-147           [-1, 2048, 8, 6]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 8, 6]           4,096\n",
            "          Conv2d-149           [-1, 2048, 8, 6]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 8, 6]           4,096\n",
            "            ReLU-151           [-1, 2048, 8, 6]               0\n",
            "      Bottleneck-152           [-1, 2048, 8, 6]               0\n",
            "          Conv2d-153            [-1, 512, 8, 6]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 8, 6]           1,024\n",
            "            ReLU-155            [-1, 512, 8, 6]               0\n",
            "          Conv2d-156            [-1, 512, 8, 6]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 8, 6]           1,024\n",
            "            ReLU-158            [-1, 512, 8, 6]               0\n",
            "          Conv2d-159           [-1, 2048, 8, 6]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 8, 6]           4,096\n",
            "            ReLU-161           [-1, 2048, 8, 6]               0\n",
            "      Bottleneck-162           [-1, 2048, 8, 6]               0\n",
            "          Conv2d-163            [-1, 512, 8, 6]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 8, 6]           1,024\n",
            "            ReLU-165            [-1, 512, 8, 6]               0\n",
            "          Conv2d-166            [-1, 512, 8, 6]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 8, 6]           1,024\n",
            "            ReLU-168            [-1, 512, 8, 6]               0\n",
            "          Conv2d-169           [-1, 2048, 8, 6]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 8, 6]           4,096\n",
            "            ReLU-171           [-1, 2048, 8, 6]               0\n",
            "      Bottleneck-172           [-1, 2048, 8, 6]               0\n",
            " ConvTranspose2d-173          [-1, 256, 16, 12]       8,388,864\n",
            "     BatchNorm2d-174          [-1, 256, 16, 12]             512\n",
            "            ReLU-175          [-1, 256, 16, 12]               0\n",
            " ConvTranspose2d-176          [-1, 256, 32, 24]       1,048,832\n",
            "     BatchNorm2d-177          [-1, 256, 32, 24]             512\n",
            "            ReLU-178          [-1, 256, 32, 24]               0\n",
            " ConvTranspose2d-179          [-1, 256, 64, 48]       1,048,832\n",
            "     BatchNorm2d-180          [-1, 256, 64, 48]             512\n",
            "            ReLU-181          [-1, 256, 64, 48]               0\n",
            "          Conv2d-182           [-1, 17, 64, 48]           4,369\n",
            "         Sigmoid-183           [-1, 17, 64, 48]               0\n",
            "================================================================\n",
            "Total params: 34,000,465\n",
            "Trainable params: 34,000,465\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.56\n",
            "Forward/backward pass size (MB): 305.11\n",
            "Params size (MB): 129.70\n",
            "Estimated Total Size (MB): 435.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IWlIpT5C4nsZ",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "criterion = my_loss\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.0001) # ,lr=0.001 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[10,100], gamma=0.1)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data_train, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(data_val, batch_size=32,\n",
        "                                          shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6467075f-d476-4505-d674-c061e5754905",
        "id": "-Nv_CXy142Bg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "print(\"device\",\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "running_loss = 0.0\n",
        "eval_iter = iter(testloader)\n",
        "net.cuda()\n",
        "print(\"start training\")\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    \n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #print(\"-\"*60)\n",
        "        #print(\"datalen\",len(data))\n",
        "        inputs, labels, validity = data\n",
        "        #print(labels.shape)\n",
        "        #for b in range(labels.shape[0]):\n",
        "        #  for h in range(labels.shape[1]):\n",
        "        #    plt.matshow(labels[b,h,:,:])\n",
        "        #print(\"L\",labels.shape)\n",
        "        #labels = labels.squeeze()\n",
        "        #print(\"l\",labels.shape)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        validity = validity.cuda()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        outputs.cuda()\n",
        "        #print(labels,labels.shape)\n",
        "        loss = criterion(outputs, labels,validity)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        #print(\"#\"*60)\n",
        "        #if i % 100 == 10:    # print every 100 mini-batches\n",
        "    scheduler.step()\n",
        "    #print(\"EPOCH\",epoch)\n",
        "    acc = get_accuracy(net,eval_iter)\n",
        "    print('EPOCH [%3d, %5d] | loss: %.7f | acc %.14f' %(epoch, i + 1, running_loss, acc ))\n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device cuda\n",
            "start training\n",
            "EPOCH [  0,  3507] | loss: 15.2324904 | acc 0.56702353154453\n",
            "EPOCH [  1,  3507] | loss: 4.7137077 | acc 0.66735544297464\n",
            "EPOCH [  2,  3507] | loss: 4.0416069 | acc 0.71747531031078\n",
            "EPOCH [  3,  3507] | loss: 3.6553278 | acc 0.73937850366937\n",
            "EPOCH [  4,  3507] | loss: 3.3486448 | acc 0.74424188391496\n",
            "EPOCH [  5,  3507] | loss: 3.0745666 | acc 0.75546536815580\n",
            "EPOCH [  6,  3507] | loss: 2.8259762 | acc 0.68888745701394\n",
            "EPOCH [  7,  3507] | loss: 2.5946723 | acc 0.73543317333868\n",
            "EPOCH [  8,  3507] | loss: 2.3773787 | acc 0.69094696007035\n",
            "EPOCH [  9,  3507] | loss: 2.1824468 | acc 0.68622314344633\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OTdfZk66UI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}